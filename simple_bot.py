from dotenv import load_dotenv
from os import getenv
from telegram import Update
from telegram.ext import (
    Updater,
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
)
from operations import (
    clean_text,
    sentiment_dict_load_and_parse,
    extract_price,
    extract_size,
    extract_tags,
)

from natasha import (
    MorphVocab,
    Doc,
    NewsEmbedding,
    NewsMorphTagger,
    NewsNERTagger,
    Segmenter,
)
from os import path
from joblib import load
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from train_intents import models_dir, vectorizer_file_name, classifier_file_name
import traceback
import logging
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker  # pip install pyspellchecker
from data.intents import intents
from data.land_plots import land_plots
from random import choice
from collections import deque


def clean_stop_words(text: str) -> str:
    "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤)"

    # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞
    words = word_tokenize(text, language="russian")
    filtered_text = [word for word in words if word not in stop_words]
    return " ".join(filtered_text)


def fix_spelling(text: str) -> str:
    "–ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–ª–æ–≤ —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏"

    words = text.split()

    corrected = []
    for word in words:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–≤–∞ –Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∏
        corrected_word = spell.correction(word) or word
        corrected.append(corrected_word)
    return " ".join(corrected)


def lemmatize_text(text: str) -> str:
    "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤ –æ–±—ã—á–Ω—É—é —Ñ–æ—Ä–º—É) —Å–ª–æ–≤"

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)  # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–µ–º–º
    if doc.tokens:
        lemmas = []
        for token in doc.tokens:
            token.lemmatize(morph_vocab)
            lemmas.append(token.lemma)

        return " ".join(lemmas)
    else:
        return text


def extract_entities(text: str) -> dict:
    "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π, –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞"

    doc = Doc(text)
    doc.segment(emb)
    doc.tag_morph(morph_vocab)
    doc.tag_ner(ner_tagger)

    entities = {}
    if doc.spans:
        for span in doc.spans:
            entities[span.type] = span.text

    entities.update(extract_price(text))
    entities.update(extract_size(text))

    # return entities
    return {k: v for k, v in entities.items() if v not in ["–ë–æ—Ç", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"]}


def analyze_sentiment(text: str) -> str:
    "–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤"
    words = text.split()
    total_score = 0
    word_count = 0

    for word in words:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ
        if word in sentiment_dict:
            total_score += sentiment_dict[word]
            word_count += 1

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
    if word_count == 0:
        return "neutral"  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É
    average_score = total_score / word_count
    if average_score > 0.65:
        return "positive"
    elif average_score < 0.35:
        return "negative"
    return "neutral"


def classify_intent(text: str):
    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π"
    vec = vectorizer.transform([text])
    return classifier.predict(vec)[0]


def process_text(text: str) -> str:
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∫–∞, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫, —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø —Å–ª–æ–≤ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è"
    text = clean_text(text)
    text = fix_spelling(text)
    text = clean_stop_words(text)
    return lemmatize_text(text)


def search_plots(entities: dict, tags: list) -> list:
    "–ü–æ–∏—Å–∫ —É—á–∞—Å—Ç–∫–æ–≤ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"
    filtered = land_plots.copy()

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ª–æ–∫–∞—Ü–∏–∏
    if "LOC" in entities:
        location = entities["LOC"].lower()
        filtered = [p for p in filtered if location in p["location"].lower()]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ü–µ–Ω–µ
    if "PRICE" in entities:
        price = entities["PRICE"]
        filtered = [p for p in filtered if p["price_value"] <= price]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É
    if "SIZE" in entities:
        size = entities["SIZE"]
        filtered = [p for p in filtered if p["size_value"] >= size]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–≥–∞–º
    if tags:
        filtered = [p for p in filtered if any(tag in p["tags"] for tag in tags)]

    return filtered


def format_short_plot_info(plot: dict) -> str:
    "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏"
    return f"""–£—á–∞—Å—Ç–æ–∫ #{plot['id']}\n\
        üìç {plot['location']} | üìè {plot['size']}\n\
        üí∞ {plot['price']}\n\
        üè∑Ô∏è –¢–µ–≥–∏: #{' #'.join(plot['tags'][:3])}\n"""


def format_land_info(plot: dict) -> str:
    "–û–ø–∏—Å–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–∞"
    return f"""–£—á–∞—Å—Ç–æ–∫ #{plot['id']}\n\
        üìç {plot['location']} | {plot['size']}\n\
        üå± –ü–æ—á–≤–∞: {plot['soil']}\n\
        üíµ –¶–µ–Ω–∞: {plot['price']}\n\
        ‚ú® –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(plot['features'])}\n\
        üìù {plot['description']}\n"""


def generate_response(
    intent: str, entities: dict, sentiment: str, user_text: str
) -> str:
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    tags = extract_tags(user_text)  # –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    if any(["–ø–æ–∏—Å–∫", "–∏—Å–∫–∞—Ç—å", "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "—Å—Ä–∞–≤–Ω–∏—Ç—å"]) in entities:
        found_plots = search_plots(entities, tags)

        if not found_plots:
            response = "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É —É—á–∞—Å—Ç–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞."
        else:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            plots_list = "\n".join([format_short_plot_info(p) for p in found_plots[:3]])
            response = f"üîç –ù–∞–π–¥–µ–Ω–æ —É—á–∞—Å—Ç–∫–æ–≤: {len(found_plots)}\n\n{plots_list}\n"
            response += "–î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∂–∏—Ç–µ ID —É—á–∞—Å—Ç–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–ü–æ–∫–∞–∂–∏ —É—á–∞—Å—Ç–æ–∫ 7')"

        return adapt_to_sentiment(response, sentiment)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–µ—Ç–∞–ª–µ–π —É—á–∞—Å—Ç–∫–∞
    if intent == "plot_details" or "—É—á–∞—Å—Ç–æ–∫" in user_text.lower():
        plot_ids = [int(word) for word in user_text.split() if word.isdigit()]
        if plot_ids:
            plot_id = plot_ids[0]
            plot = next((p for p in land_plots if p["id"] == plot_id), None)
            if plot:
                return format_land_info(plot)
            return "–£—á–∞—Å—Ç–æ–∫ —Å —Ç–∞–∫–∏–º ID –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–æ–º–µ—Ä."

    # 1. –í—ã–±–∏—Ä–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç –ø–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—é
    if intent in intents:
        response = choice(intents[intent])
    else:
        response = choice(intents["default"])

    # 2. –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π
    personalized_response = personalize_response(response, entities)

    # 3. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    return adapt_to_sentiment(personalized_response, sentiment)


def personalize_response(response: str, entities: dict) -> str:
    "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–º—è - –æ–±—Ä–∞—â–∞–µ–º—Å—è –ø–æ –∏–º–µ–Ω–∏
    if "PER" in entities:
        name = entities["PER"]
        response = f"{name}, {response[0].lower()}{response[1:]}"

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–∏ - —É—Ç–æ—á–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏
    if "LOC" in entities and any(
        keyword in response for keyword in ["–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ", "–ø–æ—Å—ë–ª–∫–µ", "—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã"]
    ):
        location = entities["LOC"]
        response = response.replace("–ø–æ—Å—ë–ª–∫–µ", f"–ø–æ—Å—ë–ª–∫–µ {location}")
        response = response.replace("–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ", f"–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ ({location})")

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ —Ü–µ–Ω–µ - —É—á–∏—Ç—ã–≤–∞–µ–º –±—é–¥–∂–µ—Ç
    if "PRICE" in entities and "—Ü–µ–Ω–∞" in response:
        price = entities["PRICE"]
        response += f" –£–ø–æ–º—è–Ω—É—Ç—ã–π –≤–∞–º–∏ –±—é–¥–∂–µ—Ç {price} –º—ã —É—á—Ç—ë–º!"

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ —Ä–∞–∑–º–µ—Ä–µ - –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    if "SIZE" in entities and "—Ä–∞–∑–º–µ—Ä" in response:
        size = entities["SIZE"]
        response = response.replace("–ö–∞–∫–æ–π —Ä–∞–∑–º–µ—Ä", f"–î–ª—è —Ä–∞–∑–º–µ—Ä–∞ {size} —Å–æ—Ç–æ–∫")

    return response


def adapt_to_sentiment(response: str, sentiment: str) -> str:
    """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ–¥ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è"""
    if sentiment == "negative":
        return "–ü–æ–Ω–∏–º–∞–µ–º –≤–∞—à–∏ —Å–æ–º–Ω–µ–Ω–∏—è. " + response + " –ú–æ–∂–µ–º —á—Ç–æ-—Ç–æ —É—Ç–æ—á–Ω–∏—Ç—å?"
    elif sentiment == "positive":
        return "–†–∞–¥—ã –≤–∞—à–µ–º—É –∏–Ω—Ç–µ—Ä–µ—Å—É! " + response
    return response


def get_context(chat_id: int, new_message: str, is_bot: bool = False) -> str:
    "–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ø–∞—Ä —Å–æ–æ–±—â–µ–Ω–∏–π)"
    if chat_id not in chat_contexts:
        chat_contexts[chat_id] = deque(maxlen=10)

    role = "–ë–æ—Ç" if is_bot else "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"

    chat_contexts[chat_id].append(f"[{role}]: {new_message}")
    return " ".join(chat_contexts[chat_id])


# ===================
# ===================

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
spell = SpellChecker(language="ru")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏–π

try:
    vectorizer = load(path.join(models_dir, vectorizer_file_name))
    classifier = load(path.join(models_dir, classifier_file_name))
except FileNotFoundError as e:
    logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ {e}\n{traceback.format_exc()}")
    raise

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ª–µ–∫—Å–∏–º–º–∏–∑–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)

morph_vocab = MorphVocab()
emb = NewsEmbedding()

segmenter = Segmenter()
morph_tagger = NewsMorphTagger(emb)

ner_tagger = NewsNERTagger(emb)

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã —Å—Ç–æ–ø-—Å–ª–æ–≤ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
stop_words = set(stopwords.words("russian"))

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
sentiment_dict = sentiment_dict_load_and_parse("./data/sentiment_dict.txt")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–∞—Ç–æ–≤

chat_contexts = {}

# ===================
# ===================


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if update.message:
        if update.effective_chat:
            chat_id = update.effective_chat.id
            if chat_id in chat_contexts:
                chat_contexts[chat_id].clear()
        await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à —á–∞—Ç-–±–æ—Ç. –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å.")


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if update.message and update.effective_chat:
        try:
            text = update.message.text  # –¢–µ–∫—Å—Ç –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            chat_id = update.effective_chat.id  # ID —á–∞—Ç–∞ —Å —ç—Ç–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º

            if text and chat_id:
                logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–ø–∏—Å–∞–ª: {text}")

                original_text = text

                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
                text = get_context(chat_id, text, is_bot=False)

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                text = process_text(text)

                # –ê–Ω–∞–ª–∏–∑
                intent = classify_intent(text)
                entities = extract_entities(original_text)
                sentiment = analyze_sentiment(text)

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —É—á—ë—Ç–æ–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                response = generate_response(intent, entities, sentiment, original_text)

                get_context(chat_id, response, is_bot=True)

                # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                await update.message.reply_text(response)
            else:
                logger.info("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–ø–∏—Å–∞–ª")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {e}\n{traceback.format_exc()}")
            await update.message.reply_text(
                "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"
            )


def run_bot():
    load_dotenv()
    token = getenv("TELEGRAM_BOT_API_TOKEN")

    if token is None or token == "":
        logger.error("–û—à–∏–±–∫–∞: —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
        exit(1)

    application = ApplicationBuilder().token(token).build()

    application.add_handler(CommandHandler("start", start))
    # application.add_handler(CommandHandler("help", help_command))
    application.add_handler(
        MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)
    )

    # Start the Bot
    application.run_polling()


if __name__ == "__main__":
    run_bot()
