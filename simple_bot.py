from dotenv import load_dotenv
from os import getenv
from telegram import Update
from telegram.ext import (
    Updater,
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    `Context`Types,
)
from operations import (
    clean_text,
    sentiment_dict_load_and_parse,
    extract_price,
    extract_size,
    extract_tags,
)

from natasha import (
    MorphVocab,
    Doc,
    NewsEmbedding,
    NewsMorphTagger,
    NewsNERTagger,
    Segmenter,
)
from os import path, remove
from joblib import load
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from train_intents import models_dir, vectorizer_file_name, classifier_file_name
import traceback
import logging
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker  # pip install pyspellchecker
from data.intents_answers import intents
from data.land_plots import land_plots
from data.advertisements import advertisements
from random import choice, random
from collections import deque
from io import BytesIO
from gtts import gTTS
import speech_recognition as sr
from pydub import AudioSegment


def clean_stop_words(text: str) -> str:
    "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤)"

    # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞
    words = word_tokenize(text, language="russian")
    filtered_text = [word for word in words if word not in stop_words]
    return " ".join(filtered_text)


def fix_spelling(text: str) -> str:
    "–ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–ª–æ–≤ —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏"

    words = text.split()

    corrected = []
    for word in words:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–≤–∞ –Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∏
        corrected_word = spell.correction(word) or word
        corrected.append(corrected_word)
    return " ".join(corrected)


def lemmatize_text(text: str) -> str:
    "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤ –æ–±—ã—á–Ω—É—é —Ñ–æ—Ä–º—É) —Å–ª–æ–≤"

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)  # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–µ–º–º
    if doc.tokens:
        lemmas = []
        for token in doc.tokens:
            token.lemmatize(morph_vocab)
            lemmas.append(token.lemma)

        return " ".join(lemmas)
    else:
        return text


def extract_entities(text: str) -> dict:
    "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π, –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞"
    segmenter = Segmenter()
    emb = NewsEmbedding()
    morph_tagger = NewsMorphTagger(emb)
    ner_tagger = NewsNERTagger(emb)

    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    doc.tag_ner(ner_tagger)

    entities = {}

    if doc.spans:
        for span in doc.spans:
            entities[span.type] = span.text

    entities.update(extract_price(text))
    entities.update(extract_size(text))

    return entities
    # return {k: v for k, v in entities.items() if v not in ["–ë–æ—Ç", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"]}


def analyze_sentiment(text: str) -> str:
    "–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤"
    words = text.split()
    total_score = 0
    word_count = 0

    for word in words:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ
        if word in sentiment_dict:
            total_score += sentiment_dict[word]
            word_count += 1

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
    if word_count == 0:
        return "neutral"  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É
    average_score = total_score / word_count
    if average_score > 0.65:
        return "positive"
    elif average_score < 0.35:
        return "negative"
    return "neutral"


def classify_intent(text: str):
    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π"
    vec = vectorizer.transform([text])
    intent = classifier.predict(vec)[0]

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    filter_keywords = {
        "—Ñ–∏–ª—å—Ç—Ä",
        "–ø–æ–¥–æ–±—Ä–∞—Ç—å",
        "–Ω–∞–π—Ç–∏",
        "–ø–æ–∏—Å–∫",
        "–≤—ã–±—Ä–∞—Ç—å",
        "–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å",
    }
    if any(keyword in text for keyword in filter_keywords):
        return "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è"

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    info_keywords = {
        "–æ–ø–∏—Å–∞–Ω–∏–µ",
        "–ø–æ–¥—Ä–æ–±–Ω–µ–µ",
        "—Ä–∞—Å—Å–∫–∞–∂–∏",
        "–¥–µ—Ç–∞–ª–∏",
        "—É—á–∞—Å—Ç–æ–∫",
        "–Ω–æ–º–µ—Ä",
        "id",
        "–ø–æ–∫–∞–∂–∏",
    }
    if any(keyword in text for keyword in info_keywords):
        return "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"

    return intent


def process_text(text: str) -> str:
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∫–∞, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫, —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø —Å–ª–æ–≤ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è"
    text = clean_text(text)
    text = fix_spelling(text)
    text = clean_stop_words(text)
    return lemmatize_text(text)


def search_plots(entities: dict, tags: list) -> list:
    "–ü–æ–∏—Å–∫ —É—á–∞—Å—Ç–∫–æ–≤ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"
    filtered = land_plots.copy()

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–≥–∞–º
    if tags:
        filtered = [p for p in filtered if any(tag in p["tags"] for tag in tags)]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ª–æ–∫–∞—Ü–∏–∏
    if "LOC" in entities:
        location = entities["LOC"].lower()
        filtered = [p for p in filtered if p["location"].lower() == location]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ü–µ–Ω–µ
    if "PRICE" in entities:
        price = entities["PRICE"]
        filtered = [p for p in filtered if p["price_value"] <= price]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É
    if "SIZE" in entities:
        size = entities["SIZE"]
        filtered = [p for p in filtered if p["size_value"] >= size]

    return filtered


def format_short_plot_info(plot: dict) -> str:
    "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏"
    return f"""–£—á–∞—Å—Ç–æ–∫ #{plot['id']}\n\
        üìç {plot['location']} | üìè {plot['size']}\n\
        üí∞ {plot['price']}\n\
        üè∑Ô∏è –¢–µ–≥–∏: #{' #'.join(plot['tags'][:3])}\n"""


def format_land_info(plot: dict) -> str:
    "–û–ø–∏—Å–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–∞"
    return f"""–£—á–∞—Å—Ç–æ–∫ #{plot['id']}\n\
        üìç {plot['location']} | {plot['size']}\n\
        üå± –ü–æ—á–≤–∞: {plot['soil']}\n\
        üíµ –¶–µ–Ω–∞: {plot['price']}\n\
        ‚ú® –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(plot['features'])}\n\
        üìù {plot['description']}\n"""


def generate_response(
    intent: str, entities: dict, sentiment: str, user_text: str
) -> str:
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–∞/—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    if intent in ["—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è", "–ø–æ–∏—Å–∫", "–∏—Å–∫–∞—Ç—å", "—Ñ–∏–ª—å—Ç—Ä"]:
        found_plots = search_plots(entities, extract_tags(user_text))

        if found_plots:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            plots_list = "\n".join([format_short_plot_info(p) for p in found_plots[:3]])
            response = f"üîç –ù–∞–π–¥–µ–Ω–æ —É—á–∞—Å—Ç–∫–æ–≤: {len(found_plots)}\n\n{plots_list}\n"
            response += "–î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∂–∏—Ç–µ ID —É—á–∞—Å—Ç–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–æ —É—á–∞—Å—Ç–æ–∫ 7')"
        else:
            response = "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É —É—á–∞—Å—Ç–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞."

        return adapt_to_sentiment(response, sentiment)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–µ—Ç–∞–ª–µ–π —É—á–∞—Å—Ç–∫–∞
    elif intent == "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è":
        land_ids = [int(word) for word in user_text.split() if word.isdigit()]
        if land_ids:
            land_id = land_ids[0]
            land = next((p for p in land_plots if p["id"] == land_id), None)
            if land:
                return format_land_info(land)
        return "–£—á–∞—Å—Ç–æ–∫ —Å —Ç–∞–∫–∏–º ID –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–æ–º–µ—Ä."

    else:
        # –í—ã–±–æ—Ä –æ—Ç–≤–µ—Ç–∞ –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        if intent in intents:
            response = choice(intents[intent])
        else:
            response = choice(intents["–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"])

        response = personalize_response(response, entities)

        response = adapt_to_sentiment(response, sentiment)

        if random() < 0.2:
            random_land = choice(land_plots)
            ad_text = choice(advertisements)
            land_info = format_short_plot_info(random_land)
            response += f"\n\n{ad_text}\n{land_info}"

        return response


def personalize_response(response: str, entities: dict) -> str:
    "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–º—è - –æ–±—Ä–∞—â–∞–µ–º—Å—è –ø–æ –∏–º–µ–Ω–∏
    if "PER" in entities:
        name = entities["PER"]
        response = f"{name}, {response[0].lower()}{response[1:]}"

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–∏ - —É—Ç–æ—á–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏
    if "LOC" in entities and any(
        keyword in response for keyword in ["–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ", "–ø–æ—Å—ë–ª–∫–µ", "—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã"]
    ):
        location = entities["LOC"]
        response = response.replace("–ø–æ—Å—ë–ª–∫–µ", f"–ø–æ—Å—ë–ª–∫–µ {location}")
        response = response.replace("–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ", f"–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ ({location})")

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ —Ü–µ–Ω–µ - —É—á–∏—Ç—ã–≤–∞–µ–º –±—é–¥–∂–µ—Ç
    if "PRICE" in entities and "—Ü–µ–Ω–∞" in response:
        price = entities["PRICE"]
        response += f" –£–ø–æ–º—è–Ω—É—Ç—ã–π –≤–∞–º–∏ –±—é–¥–∂–µ—Ç {price} –º—ã —É—á—Ç—ë–º!"

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ —Ä–∞–∑–º–µ—Ä–µ - –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    if "SIZE" in entities and "—Ä–∞–∑–º–µ—Ä" in response:
        size = entities["SIZE"]
        response = response.replace("–ö–∞–∫–æ–π —Ä–∞–∑–º–µ—Ä", f"–î–ª—è —Ä–∞–∑–º–µ—Ä–∞ {size} —Å–æ—Ç–æ–∫")

    return response


def adapt_to_sentiment(response: str, sentiment: str) -> str:
    "–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ–¥ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è"
    if sentiment == "negative":
        return "–ü–æ–Ω–∏–º–∞–µ–º –≤–∞—à–∏ —Å–æ–º–Ω–µ–Ω–∏—è. " + response + " –ú–æ–∂–µ–º —á—Ç–æ-—Ç–æ —É—Ç–æ—á–Ω–∏—Ç—å?"
    elif sentiment == "positive":
        return "–†–∞–¥—ã –≤–∞—à–µ–º—É –∏–Ω—Ç–µ—Ä–µ—Å—É! " + response
    return response


# ===================
# ===================

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
spell = SpellChecker(language="ru")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏–π

try:
    vectorizer = load(path.join(models_dir, vectorizer_file_name))
    classifier = load(path.join(models_dir, classifier_file_name))
except FileNotFoundError as e:
    logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ {e}\n{traceback.format_exc()}")
    raise

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ª–µ–∫—Å–∏–º–º–∏–∑–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)

emb = NewsEmbedding()
segmenter = Segmenter()
morph_tagger = NewsMorphTagger(emb)
ner_tagger = NewsNERTagger(emb)

morph_vocab = MorphVocab()

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã —Å—Ç–æ–ø-—Å–ª–æ–≤ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
stop_words = set(stopwords.words("russian"))

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
sentiment_dict = sentiment_dict_load_and_parse("./data/sentiment_dict.txt")


# ===================
# ===================


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if update.message and update.effective_chat:
        chat_id = update.effective_chat.id

        await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à —á–∞—Ç-–±–æ—Ç. –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å.")


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if update.message and update.effective_chat:
        try:
            is_voice = False

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            if update.message.voice:
                is_voice = True
                voice_file = await update.message.voice.get_file()
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ–º –≥–æ–ª–æ—Å
                await voice_file.download_to_drive("voice.ogg")
                audio = AudioSegment.from_ogg("voice.ogg")
                audio.export("voice.wav", format="wav")
                recognizer = sr.Recognizer()
                with sr.AudioFile("voice.wav") as source:
                    audio_data = recognizer.record(source)
                text = str(recognizer.recognize_google(audio_data, language="ru-RU"))
                remove("voice.ogg")
                remove("voice.wav")
            else:
                text = update.message.text

            chat_id = update.effective_chat.id
            logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–ø–∏—Å–∞–ª: {text}")

            if text:
                text = process_text(text)

                intent = classify_intent(text)
                entities = extract_entities(text)
                sentiment = analyze_sentiment(text)
                logger.info(
                    f"\n–ù–∞–º–µ—Ä–µ–Ω–∏–µ: {intent}\n–°—É—â–Ω–æ—Å—Ç–∏: {entities}\n–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}"
                )

                response = generate_response(intent, entities, sentiment, text)
                logger.info(f"–û—Ç–≤–µ—Ç –±–æ—Ç–∞: {response}")

                if is_voice:
                    tts = gTTS(text=response, lang="ru")
                    voice_io = BytesIO()
                    tts.write_to_fp(voice_io)
                    voice_io.seek(0)
                    await update.message.reply_voice(voice=voice_io)
                else:
                    # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                    await update.message.reply_text(response)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {e}\n{traceback.format_exc()}")
            await update.message.reply_text(
                "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"
            )


def run_bot():
    load_dotenv()
    token = getenv("TELEGRAM_BOT_API_TOKEN")

    if token is None or token == "":
        logger.error("–û—à–∏–±–∫–∞: —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
        exit(1)

    application = ApplicationBuilder().token(token).build()

    application.add_handler(CommandHandler("start", start))
    # application.add_handler(CommandHandler("help", help_command))
    application.add_handler(
        MessageHandler(
            (filters.TEXT | filters.VOICE) & ~filters.COMMAND, handle_message
        )
    )

    application.run_polling()


if __name__ == "__main__":
    run_bot()
